# Simple Streaming Chat API

This document describes the simplified streaming chat functionality implemented in the Aeva backend.

## Overview

The streaming chat API provides real-time streaming responses for chat interactions, improving user experience by showing responses as they are generated rather than waiting for the complete response.

## Architecture

### Services

**StreamingService** (`app/services/streaming_service.py`)
- Single service that handles all streaming functionality
- Integrates with existing services (embedding, web search, LLM)
- Manages Server-Sent Events (SSE) for real-time streaming
- Handles both study material and web search streaming

### API Endpoints

- **POST** `/chat/stream` - Stream chat responses
- **POST** `/chat/` - Regular chat responses (non-streaming)
- **GET** `/chat/stats` - Get chat statistics

## Usage

### Streaming Chat Endpoint

**Study Material Search with Specific PDFs:**
```bash
curl -X POST "http://localhost:8000/chat/stream" \
  -H "Content-Type: application/json" \
  -H "user-id: your-user-id" \
  -d '{
    "message": "What is artificial intelligence?",
    "search_mode": "study_material",
    "n_results": 3,
    "pdf_names": ["document1.pdf", "document2.pdf"]
  }'
```

**Web Search (no PDF names needed):**
```bash
curl -X POST "http://localhost:8000/chat/stream" \
  -H "Content-Type: application/json" \
  -H "user-id: your-user-id" \
  -d '{
    "message": "What is artificial intelligence?",
    "search_mode": "web_search",
    "n_results": 3
  }'
```

### Response Format

The streaming endpoint returns Server-Sent Events (SSE) with the following format:

```json
// Content chunks
data: {"type": "chunk", "content": "Artificial intelligence (AI) is...", "answer_source": "web_search"}

// End signal with metadata
data: {"type": "end", "answer_source": "web_search", "relevant_chunks": [...], "metadata": [...]}

// Error response
data: {"type": "error", "content": "Error message"}
```

## Features

### 1. Real-time Streaming
- Responses are streamed as they are generated by the LLM
- Users see content appearing progressively
- Better perceived performance and user experience

### 2. Support for Both Search Modes
- **Study Material**: Streams responses based on uploaded documents
  - Can specify specific PDF names to search in (`pdf_names` array)
  - If no PDF names provided, searches in all user's documents
- **Web Search**: Streams responses based on web search results

### 3. Error Handling
- Graceful error handling with streaming error messages
- Continues streaming even if some operations fail

### 4. Metadata Preservation
- Maintains context and metadata information
- Provides source information for responses

## Implementation Details

### Streaming Flow

1. **Request Processing**: Validates incoming chat request
2. **Search Mode Routing**: Routes to appropriate streaming handler
3. **Context Building**: Builds search context based on mode
4. **LLM Streaming**: Streams response from LLM API
5. **Response Formatting**: Formats chunks and end signals
6. **SSE Delivery**: Delivers formatted responses as Server-Sent Events

### Dependencies

- `aiohttp`: For async HTTP requests to LLM API
- `fastapi`: For API framework and streaming responses
- Existing services: Embedding, Web Search, LLM services

## Testing

Use the provided test script to test streaming functionality:

```bash
python test_streaming.py
```

## Benefits

1. **Improved UX**: Users see responses as they're generated
2. **Better Performance**: Perceived faster response times
3. **Real-time Interaction**: More engaging chat experience
4. **Scalability**: Efficient resource usage with streaming
5. **Compatibility**: Works with existing chat infrastructure

## Future Enhancements

1. **Streaming Query Classification**: Stream the query classification process
2. **Chunked Context**: Stream context building for large documents
3. **Progress Indicators**: Add progress information to streams
4. **Rate Limiting**: Implement streaming rate limiting
5. **Connection Management**: Better handling of dropped connections
